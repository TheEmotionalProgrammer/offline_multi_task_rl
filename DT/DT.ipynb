{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.10.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(r'C:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl')\n",
    "\n",
    "import gymnasium as gym\n",
    "import dill\n",
    "\n",
    "from four_room.env import FourRoomsEnv\n",
    "from four_room.wrappers import gym_wrapper\n",
    "from four_room.shortest_path import find_all_action_values\n",
    "from four_room.utils import obs_to_state\n",
    "from four_room_extensions import fourrooms_dataset_gen\n",
    "from d3rlpy.algos import DiscreteDecisionTransformerConfig\n",
    "from d3rlpy.metrics import EnvironmentEvaluator, TDErrorEvaluator, DiscreteActionMatchEvaluator, evaluate_transformer_with_environment\n",
    "from d3rlpy.datasets import MDPDataset\n",
    "from d3rlpy.logging import WanDBAdapterFactory\n",
    "from d3rlpy.ope import FQEConfig, DiscreteFQE\n",
    "from d3rlpy import load_learnable\n",
    "import wandb\n",
    "import numpy as np\n",
    "import utils\n",
    "from datetime import datetime\n",
    "import imageio\n",
    "import torch\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from utils import get_DQN_checkpoints\n",
    "from four_room_extensions.fourrooms_dataset_gen import get_mixed_policy_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config_path = 'train'\n",
    "reachable_test_config_path = 'test_100'\n",
    "unreachable_test_config_path = 'test_0'\n",
    "best_policy = True\n",
    "episode_length = [0, 25, 50, 75, 100]\n",
    "device = True if torch.cuda.is_available() else None\n",
    "n_epochs = 100\n",
    "n_steps_per_epoch = 50\n",
    "wandb_explanation = f\"train on mixed {episode_length} with best_policy={best_policy}, test on (train, test_100, test_0)\"\n",
    "render = False\n",
    "wandb_project_name = \"DT_mixed\"\n",
    "wandb_run_name = f\"DT_mixed_run_before_midterm\"\n",
    "DQN_models_path = os.path.join(\"..\", \"four_room_extensions\", \"DQN_models\", \"performance_per_model.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\lib\\site-packages\\gymnasium\\envs\\registration.py:694: UserWarning: WARN: Overriding environment MiniGrid-FourRooms-v1 already in registry.\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "c:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\lib\\site-packages\\gymnasium\\core.py:311: UserWarning: WARN: env.width to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.width` for environment variables or `env.get_wrapper_attr('width')` that will search the reminding wrappers.\n",
      "  logger.warn(\n",
      "c:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\lib\\site-packages\\gymnasium\\core.py:311: UserWarning: WARN: env.height to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.height` for environment variables or `env.get_wrapper_attr('height')` that will search the reminding wrappers.\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-02 19:57.01 [info     ] Signatures have been automatically determined. action_signature=Signature(dtype=[dtype('int64')], shape=[(1,)]) observation_signature=Signature(dtype=[dtype('uint8')], shape=[(324,)]) reward_signature=Signature(dtype=[dtype('int32')], shape=[(1,)])\n",
      "2024-06-02 19:57.01 [info     ] Action-space has been automatically determined. action_space=<ActionSpace.DISCRETE: 2>\n",
      "2024-06-02 19:57.01 [info     ] Action size has been automatically determined. action_size=3\n",
      "2024-06-02 19:57.03 [info     ] Signatures have been automatically determined. action_signature=Signature(dtype=[dtype('int64')], shape=[(1,)]) observation_signature=Signature(dtype=[dtype('uint8')], shape=[(324,)]) reward_signature=Signature(dtype=[dtype('int32')], shape=[(1,)])\n",
      "2024-06-02 19:57.03 [info     ] Action-space has been automatically determined. action_space=<ActionSpace.DISCRETE: 2>\n",
      "2024-06-02 19:57.03 [info     ] Action size has been automatically determined. action_size=3\n",
      "2024-06-02 19:57.06 [info     ] Signatures have been automatically determined. action_signature=Signature(dtype=[dtype('int64')], shape=[(1,)]) observation_signature=Signature(dtype=[dtype('uint8')], shape=[(324,)]) reward_signature=Signature(dtype=[dtype('int32')], shape=[(1,)])\n",
      "2024-06-02 19:57.06 [info     ] Action-space has been automatically determined. action_space=<ActionSpace.DISCRETE: 2>\n",
      "2024-06-02 19:57.06 [info     ] Action size has been automatically determined. action_size=3\n",
      "policies [100, 75, 50, 25, 0]: [('470000', 9.3), ('450000', 13.68), ('390000', 26.98), ('350000', 38.3), ('300000', 54.48)]\n",
      "Data policy: 300000, Num_transitions: 434\n",
      "Data policy: 350000, Num_transitions: 251\n",
      "Data policy: 390000, Num_transitions: 292\n",
      "Data policy: 450000, Num_transitions: 80\n",
      "Data policy: 470000, Num_transitions: 112\n",
      "2024-06-02 19:57.11 [info     ] Signatures have been automatically determined. action_signature=Signature(dtype=[dtype('int64')], shape=[(1,)]) observation_signature=Signature(dtype=[dtype('uint8')], shape=[(324,)]) reward_signature=Signature(dtype=[dtype('int32')], shape=[(1,)])\n",
      "2024-06-02 19:57.11 [info     ] Action-space has been automatically determined. action_space=<ActionSpace.DISCRETE: 2>\n",
      "2024-06-02 19:57.11 [info     ] Action size has been automatically determined. action_size=3\n"
     ]
    }
   ],
   "source": [
    "train_config = fourrooms_dataset_gen.get_config(train_config_path)\n",
    "train_dataset, train_env, tasks_finished, tasks_failed = fourrooms_dataset_gen.get_expert_dataset_from_config(train_config, render=render, render_name=\"DT_train_expert\")\n",
    "\n",
    "train_dataset = MDPDataset(\n",
    "    observations=train_dataset.get(\"observations\"),\n",
    "    actions=train_dataset.get(\"actions\"),\n",
    "    rewards=train_dataset.get(\"rewards\"),\n",
    "    terminals=train_dataset.get(\"terminals\"),\n",
    ")\n",
    "\n",
    "test_config_reachable = fourrooms_dataset_gen.get_config(reachable_test_config_path)\n",
    "test_dataset_reachable, test_env_reachable, tasks_finished, tasks_failed = fourrooms_dataset_gen.get_expert_dataset_from_config(test_config_reachable, render=render, render_name=\"DT_test_expert_reachable\")\n",
    "\n",
    "test_dataset_reachable = MDPDataset(\n",
    "    observations=test_dataset_reachable.get(\"observations\"),\n",
    "    actions=test_dataset_reachable.get(\"actions\"),\n",
    "    rewards=test_dataset_reachable.get(\"rewards\"),\n",
    "    terminals=test_dataset_reachable.get(\"terminals\"),\n",
    ")\n",
    "\n",
    "test_config_unreachable = fourrooms_dataset_gen.get_config(unreachable_test_config_path)\n",
    "test_dataset_unreachable, test_env_unreachable, tasks_finished, tasks_failed = fourrooms_dataset_gen.get_expert_dataset_from_config(test_config_unreachable, render=render, render_name=\"DT_test_expert_unreachable\")\n",
    "\n",
    "test_dataset_unreachable = MDPDataset(\n",
    "    observations=test_dataset_unreachable.get(\"observations\"),\n",
    "    actions=test_dataset_unreachable.get(\"actions\"),\n",
    "    rewards=test_dataset_unreachable.get(\"rewards\"),\n",
    "    terminals=test_dataset_unreachable.get(\"terminals\"),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "checkpoints = get_DQN_checkpoints(DQN_models_path, episode_length, best_policy=best_policy)\n",
    "mixed_dataset, finished, failed = get_mixed_policy_dataset(train_config, train_env, checkpoints)\n",
    "mixed_dataset = MDPDataset(\n",
    "    observations=mixed_dataset.get(\"observations\"),\n",
    "    actions=mixed_dataset.get(\"actions\"),\n",
    "    rewards=mixed_dataset.get(\"rewards\"),\n",
    "    terminals=mixed_dataset.get(\"terminals\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_saver_d3rlpy_callback(algo, epoch, total_step, n_epochs, n_steps_per_epoch, title_addition = \"\"):\n",
    "    \"\"\"\n",
    "    Callback to save the model at the end of each epoch\n",
    "\n",
    "    Args:\n",
    "        algo: The algorithm object\n",
    "        epoch: The current epoch\n",
    "        total_step: The total number of steps taken so far\n",
    "        n_epochs: The total number of epochs\n",
    "        n_steps_per_epoch: The number of steps in each epoch\n",
    "    \"\"\"\n",
    "    if total_step % n_steps_per_epoch == 0:\n",
    "        algo.save(f\"dt_{title_addition}_model_at_step_{total_step}_{datetime.now().strftime('%Y%m%d-%H%M%S')}.d3\")\n",
    "        \n",
    "def eval_model(policy, env, n_episodes):\n",
    "    total_reward = 0\n",
    "    n_steps_taken = 0\n",
    "    for _ in range(n_episodes):\n",
    "        policy.reset()\n",
    "        observation, reward = env.reset()[0], 0.0\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            # take action\n",
    "            n_steps_taken += 1\n",
    "            action = policy.predict(observation, reward)\n",
    "\n",
    "            observation, _reward, terminated, truncated, _ = env.step(action)\n",
    "            reward = float(_reward)\n",
    "            total_reward += reward\n",
    "            done = terminated or truncated\n",
    "\n",
    "        # episode_rewards.append(episode_reward)\n",
    "    return total_reward / n_episodes, n_steps_taken / n_episodes\n",
    "\n",
    "# dt = load_learnable(\"/kaggle/input/model-checkpoints/dt_model_at_epoch_1_20240522-124018.d3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-02 19:57.34 [info     ] dataset info                   dataset_info=DatasetInfo(observation_signature=Signature(dtype=[dtype('uint8')], shape=[(324,)]), action_signature=Signature(dtype=[dtype('int64')], shape=[(1,)]), reward_signature=Signature(dtype=[dtype('int32')], shape=[(1,)]), action_space=<ActionSpace.DISCRETE: 2>, action_size=3)\n",
      "2024-06-02 19:57.34 [info     ] Directory is created at d3rlpy_logs\\DiscreteDecisionTransformer_20240602195734\n",
      "2024-06-02 19:57.34 [debug    ] Building models...            \n",
      "2024-06-02 19:57.35 [debug    ] Models have been built.       \n",
      "2024-06-02 19:57.35 [info     ] Parameters                     params={'observation_shape': [324], 'action_size': 3, 'config': {'type': 'discrete_decision_transformer', 'params': {'batch_size': 128, 'gamma': 0.99, 'observation_scaler': {'type': 'none', 'params': {}}, 'action_scaler': {'type': 'none', 'params': {}}, 'reward_scaler': {'type': 'none', 'params': {}}, 'context_size': 20, 'max_timestep': 1000, 'learning_rate': 0.0006, 'encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'optim_factory': {'type': 'adam', 'params': {'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}}, 'num_heads': 8, 'num_layers': 6, 'attn_dropout': 0.1, 'resid_dropout': 0.1, 'embed_dropout': 0.1, 'activation_type': 'gelu', 'embed_activation_type': 'tanh', 'position_encoding_type': <PositionEncodingType.GLOBAL: 'global'>, 'warmup_tokens': 10240, 'final_tokens': 30000000, 'clip_grad_norm': 1.0, 'compile': False}}}\n",
      "2024-06-02 19:58.54 [info     ] DiscreteDecisionTransformer_20240602195734: epoch=1 step=20 epoch=1 metrics={'time_sample_batch': 0.024263489246368408, 'time_algorithm_update': 3.892080879211426, 'loss': 1.005778393149376, 'learning_rate': 0.0005403716328765725, 'time_step': 3.9163949847221375} step=20\n",
      "2024-06-02 19:58.54 [info     ] Model parameters are saved to d3rlpy_logs\\DiscreteDecisionTransformer_20240602195734\\model_20.d3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [07:03<11:38:46, 423.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-02 20:04.38 [info     ] dataset info                   dataset_info=DatasetInfo(observation_signature=Signature(dtype=[dtype('uint8')], shape=[(324,)]), action_signature=Signature(dtype=[dtype('int64')], shape=[(1,)]), reward_signature=Signature(dtype=[dtype('int32')], shape=[(1,)]), action_space=<ActionSpace.DISCRETE: 2>, action_size=3)\n",
      "2024-06-02 20:04.38 [info     ] Directory is created at d3rlpy_logs\\DiscreteDecisionTransformer_20240602200438\n",
      "2024-06-02 20:04.38 [warning  ] Skip building models since they're already built.\n",
      "2024-06-02 20:04.38 [info     ] Parameters                     params={'observation_shape': [324], 'action_size': 3, 'config': {'type': 'discrete_decision_transformer', 'params': {'batch_size': 128, 'gamma': 0.99, 'observation_scaler': {'type': 'none', 'params': {}}, 'action_scaler': {'type': 'none', 'params': {}}, 'reward_scaler': {'type': 'none', 'params': {}}, 'context_size': 20, 'max_timestep': 1000, 'learning_rate': 0.0006, 'encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'optim_factory': {'type': 'adam', 'params': {'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}}, 'num_heads': 8, 'num_layers': 6, 'attn_dropout': 0.1, 'resid_dropout': 0.1, 'embed_dropout': 0.1, 'activation_type': 'gelu', 'embed_activation_type': 'tanh', 'position_encoding_type': <PositionEncodingType.GLOBAL: 'global'>, 'warmup_tokens': 10240, 'final_tokens': 30000000, 'clip_grad_norm': 1.0, 'compile': False}}}\n",
      "2024-06-02 20:06.12 [info     ] DiscreteDecisionTransformer_20240602200438: epoch=1 step=20 epoch=1 metrics={'time_sample_batch': 0.025698983669281007, 'time_algorithm_update': 4.6402752041816715, 'loss': 0.6712123394012451, 'learning_rate': 0.0005999953395599793, 'time_step': 4.665974187850952} step=20\n",
      "2024-06-02 20:06.12 [info     ] Model parameters are saved to d3rlpy_logs\\DiscreteDecisionTransformer_20240602200438\\model_20.d3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [14:56<12:19:36, 452.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-02 20:12.31 [info     ] dataset info                   dataset_info=DatasetInfo(observation_signature=Signature(dtype=[dtype('uint8')], shape=[(324,)]), action_signature=Signature(dtype=[dtype('int64')], shape=[(1,)]), reward_signature=Signature(dtype=[dtype('int32')], shape=[(1,)]), action_space=<ActionSpace.DISCRETE: 2>, action_size=3)\n",
      "2024-06-02 20:12.31 [info     ] Directory is created at d3rlpy_logs\\DiscreteDecisionTransformer_20240602201231\n",
      "2024-06-02 20:12.31 [warning  ] Skip building models since they're already built.\n",
      "2024-06-02 20:12.31 [info     ] Parameters                     params={'observation_shape': [324], 'action_size': 3, 'config': {'type': 'discrete_decision_transformer', 'params': {'batch_size': 128, 'gamma': 0.99, 'observation_scaler': {'type': 'none', 'params': {}}, 'action_scaler': {'type': 'none', 'params': {}}, 'reward_scaler': {'type': 'none', 'params': {}}, 'context_size': 20, 'max_timestep': 1000, 'learning_rate': 0.0006, 'encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'optim_factory': {'type': 'adam', 'params': {'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}}, 'num_heads': 8, 'num_layers': 6, 'attn_dropout': 0.1, 'resid_dropout': 0.1, 'embed_dropout': 0.1, 'activation_type': 'gelu', 'embed_activation_type': 'tanh', 'position_encoding_type': <PositionEncodingType.GLOBAL: 'global'>, 'warmup_tokens': 10240, 'final_tokens': 30000000, 'clip_grad_norm': 1.0, 'compile': False}}}\n",
      "2024-06-02 20:14.00 [info     ] DiscreteDecisionTransformer_20240602201231: epoch=1 step=20 epoch=1 metrics={'time_sample_batch': 0.024856972694396972, 'time_algorithm_update': 4.384004294872284, 'loss': 0.5562082529067993, 'learning_rate': 0.0005999854652450401, 'time_step': 4.408963751792908} step=20\n",
      "2024-06-02 20:14.01 [info     ] Model parameters are saved to d3rlpy_logs\\DiscreteDecisionTransformer_20240602201231\\model_20.d3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [16:29<13:28:16, 494.86s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 27\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n_epochs)):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# n_steps = n_steps_per_epoch, because we want to do manual evaluations\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# n_steps_per_epoch = n_steps_per_epoch, because we want to save the model at the end of each epoch with the callback\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     dt\u001b[38;5;241m.\u001b[39mfit(mixed_dataset, n_steps\u001b[38;5;241m=\u001b[39mn_steps_per_epoch, n_steps_per_epoch\u001b[38;5;241m=\u001b[39mn_steps_per_epoch, callback\u001b[38;5;241m=\u001b[39mmodel_saver_d3rlpy_callback_partial, show_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 27\u001b[0m     train_eval_score, train_num_steps \u001b[38;5;241m=\u001b[39m \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_stateful_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_return\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtopologies\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43mtrain_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtopologies\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     test_reachable_eval_score, test_reachable_num_steps \u001b[38;5;241m=\u001b[39m eval_model(dt\u001b[38;5;241m.\u001b[39mas_stateful_wrapper(target_return\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopologies\u001b[39m\u001b[38;5;124m\"\u001b[39m]), action_sampler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m     32\u001b[0m                                                            test_env_reachable,\n\u001b[0;32m     33\u001b[0m                                                            \u001b[38;5;28mlen\u001b[39m(test_config_reachable[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopologies\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     34\u001b[0m                                                            )\n\u001b[0;32m     35\u001b[0m     test_unreachable_eval_score, test_unreachable_num_steps \u001b[38;5;241m=\u001b[39m eval_model(dt\u001b[38;5;241m.\u001b[39mas_stateful_wrapper(target_return\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopologies\u001b[39m\u001b[38;5;124m\"\u001b[39m]), action_sampler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m     36\u001b[0m                                                             test_env_unreachable,\n\u001b[0;32m     37\u001b[0m                                                             \u001b[38;5;28mlen\u001b[39m(test_config_unreachable[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopologies\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     38\u001b[0m                                                             )\n",
      "Cell \u001b[1;32mIn[12], line 27\u001b[0m, in \u001b[0;36meval_model\u001b[1;34m(policy, env, n_episodes)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# take action\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     n_steps_taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 27\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     observation, _reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     30\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(_reward)\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\lib\\site-packages\\d3rlpy\\algos\\transformer\\base.py:176\u001b[0m, in \u001b[0;36mStatefulTransformerWrapper.predict\u001b[1;34m(self, x, reward)\u001b[0m\n\u001b[0;32m    164\u001b[0m     numpy_observations \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    165\u001b[0m         np\u001b[38;5;241m.\u001b[39marray([o[i] \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_observations])\n\u001b[0;32m    166\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x))\n\u001b[0;32m    167\u001b[0m     ]\n\u001b[0;32m    169\u001b[0m inpt \u001b[38;5;241m=\u001b[39m TransformerInput(\n\u001b[0;32m    170\u001b[0m     observations\u001b[38;5;241m=\u001b[39mnumpy_observations,\n\u001b[0;32m    171\u001b[0m     actions\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actions),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    174\u001b[0m     timesteps\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timesteps),\n\u001b[0;32m    175\u001b[0m )\n\u001b[1;32m--> 176\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_sampler(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_algo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actions[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m action\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actions\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_pad_action())\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\lib\\site-packages\\d3rlpy\\algos\\transformer\\base.py:371\u001b[0m, in \u001b[0;36mTransformerAlgoBase.predict\u001b[1;34m(self, inpt)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    363\u001b[0m     torch_inpt \u001b[38;5;241m=\u001b[39m TorchTransformerInput\u001b[38;5;241m.\u001b[39mfrom_numpy(\n\u001b[0;32m    364\u001b[0m         inpt\u001b[38;5;241m=\u001b[39minpt,\n\u001b[0;32m    365\u001b[0m         context_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mcontext_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    369\u001b[0m         reward_scaler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mreward_scaler,\n\u001b[0;32m    370\u001b[0m     )\n\u001b[1;32m--> 371\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_impl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch_inpt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39maction_scaler:\n\u001b[0;32m    374\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39maction_scaler\u001b[38;5;241m.\u001b[39mreverse_transform(action)\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\lib\\site-packages\\d3rlpy\\torch_utility.py:388\u001b[0m, in \u001b[0;36meval_api.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules, Modules)\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mset_eval()\n\u001b[1;32m--> 388\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\lib\\site-packages\\d3rlpy\\algos\\transformer\\base.py:54\u001b[0m, in \u001b[0;36mTransformerAlgoImplBase.predict\u001b[1;34m(self, inpt)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;129m@eval_api\u001b[39m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, inpt: TorchTransformerInput) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\lib\\site-packages\\d3rlpy\\algos\\transformer\\torch\\decision_transformer_impl.py:131\u001b[0m, in \u001b[0;36mDiscreteDecisionTransformerImpl.inner_predict\u001b[1;34m(self, inpt)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, inpt: TorchTransformerInput) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;66;03m# (1, T, A)\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m     _, logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43minpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturns_to_go\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimesteps\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;66;03m# (1, T, A) -> (A,)\u001b[39;00m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\lib\\site-packages\\d3rlpy\\models\\torch\\transformers.py:418\u001b[0m, in \u001b[0;36mDiscreteDecisionTransformer.forward\u001b[1;34m(self, x, action, return_to_go, timesteps)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m    416\u001b[0m     h \u001b[38;5;241m=\u001b[39m h[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m--> 418\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gpt2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# use state embeddings as input\u001b[39;00m\n\u001b[0;32m    421\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output(h[:, \u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m3\u001b[39m, :])\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\lib\\site-packages\\d3rlpy\\models\\torch\\transformers.py:239\u001b[0m, in \u001b[0;36mGPT2.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    238\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dropout(x)\n\u001b[1;32m--> 239\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layer_norm(h)\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m h\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\lib\\site-packages\\d3rlpy\\models\\torch\\transformers.py:153\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    152\u001b[0m     norm_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layer_norm1(x)\n\u001b[1;32m--> 153\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m     norm_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layer_norm2(x)\n\u001b[0;32m    155\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mlp(norm_x)\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\lib\\site-packages\\d3rlpy\\models\\torch\\transformers.py:68\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     66\u001b[0m shape \u001b[38;5;241m=\u001b[39m (batch_size, context_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_heads, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     67\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_k(x)\u001b[38;5;241m.\u001b[39mview(shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_q\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     69\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_v(x)\u001b[38;5;241m.\u001b[39mview(shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# (B, H, T, N / H) -> (B, H, T, T)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize neural networks with the given observation shape and action size.\n",
    "# this is not necessary when you directly call fit or fit_online method.\n",
    "# dt.build_with_dataset(dataset)\n",
    "\n",
    "wandb_config = {\"explanation\": wandb_explanation,\n",
    "                \"n_epochs\": n_epochs,\n",
    "                \"n_steps_per_epoch\": n_steps_per_epoch,\n",
    "                \"episode length\": episode_length,\n",
    "                \"checkpoints\": checkpoints,\n",
    "                \"best_policy\": best_policy,\n",
    "                }\n",
    "\n",
    "dt = DiscreteDecisionTransformerConfig().create(device=device)\n",
    "\n",
    "model_saver_d3rlpy_callback_partial = partial(model_saver_d3rlpy_callback, n_epochs=n_epochs, n_steps_per_epoch=n_steps_per_epoch, title_addition=wandb_explanation)\n",
    "\n",
    "train_env = utils.ObservationFlattenerWrapper(train_env)\n",
    "test_env_reachable = utils.ObservationFlattenerWrapper(test_env_reachable)\n",
    "test_env_unreachable = utils.ObservationFlattenerWrapper(test_env_unreachable)\n",
    "\n",
    "with wandb.init(project=wandb_project_name, name=wandb_run_name, config=wandb_config):\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        # n_steps = n_steps_per_epoch, because we want to do manual evaluations\n",
    "        # n_steps_per_epoch = n_steps_per_epoch, because we want to save the model at the end of each epoch with the callback\n",
    "        dt.fit(mixed_dataset, n_steps=n_steps_per_epoch, n_steps_per_epoch=n_steps_per_epoch, callback=model_saver_d3rlpy_callback_partial, show_progress=False, save_interval=1000)\n",
    "\n",
    "        train_eval_score, train_num_steps = eval_model(dt.as_stateful_wrapper(target_return=1, action_sampler=None),\n",
    "                                                                 train_env,\n",
    "                                                                 len(train_config[\"topologies\"])\n",
    "                                                                 )\n",
    "        test_reachable_eval_score, test_reachable_num_steps = eval_model(dt.as_stateful_wrapper(target_return=1, action_sampler=None),\n",
    "                                                               test_env_reachable,\n",
    "                                                               len(test_config_reachable[\"topologies\"])\n",
    "                                                               )\n",
    "        test_unreachable_eval_score, test_unreachable_num_steps = eval_model(dt.as_stateful_wrapper(target_return=1, action_sampler=None),\n",
    "                                                                test_env_unreachable,\n",
    "                                                                len(test_config_unreachable[\"topologies\"])\n",
    "                                                                )\n",
    "        \n",
    "        wandb.log({\"Cumulative Reward\": {\"Train\": train_eval_score, \"Test_reachable\": test_reachable_eval_score, \"Test_unreachable\": test_unreachable_eval_score}}, step=(epoch+1) * n_steps_per_epoch)\n",
    "        wandb.log({\"Number of steps taken\": {\"Train\": train_num_steps, \"Test_reachable\": test_reachable_num_steps, \"Test_unreachable\": test_unreachable_num_steps}}, step=(epoch+1) * n_steps_per_epoch)\n",
    "        \n",
    "\n",
    "\n",
    "        # testing with d3rlpy library\n",
    "        # train_eval_score = evaluate_transformer_with_environment(\n",
    "        #                 algo=dt.as_stateful_wrapper(\n",
    "        #                     target_return=len(train_config[\"topologies\"]),\n",
    "        #                     action_sampler=None,\n",
    "        #                 ),\n",
    "        #                 env=train_env,\n",
    "        #                 n_trials=len(train_config[\"topologies\"]),\n",
    "        #             )\n",
    "        \n",
    "        # test_reachable_eval_score = evaluate_transformer_with_environment(\n",
    "        #                 algo=dt.as_stateful_wrapper(\n",
    "        #                     target_return=len(test_config_reachable[\"topologies\"]),\n",
    "        #                     action_sampler=None,\n",
    "        #                 ),\n",
    "        #                 env=test_env_reachable,\n",
    "        #                 n_trials=len(test_config_reachable[\"topologies\"]),\n",
    "        #             )\n",
    "        \n",
    "        # test_unreachable_eval_score = evaluate_transformer_with_environment(\n",
    "        #                 algo=dt.as_stateful_wrapper(\n",
    "        #                     target_return=len(test_config_unreachable[\"topologies\"]),\n",
    "        #                     action_sampler=None,\n",
    "        #                 ),\n",
    "        #                 env=test_env_unreachable,\n",
    "        #                 n_trials=len(test_config_unreachable[\"topologies\"]),\n",
    "        #             )\n",
    "        \n",
    "\n",
    "\n",
    "# offline training\n",
    "# dt.fit(train_dataset,\n",
    "#         n_steps=n_steps,\n",
    "#         n_steps_per_epoch=n_steps_per_epoch,\n",
    "#         eval_env=train_env,\n",
    "#         eval_target_return=0,\n",
    "#         logger_adapter=WanDBAdapterFactory(project=\"DT\"),\n",
    "#         callback=model_saver_d3rlpy_callback_partial)\n",
    "\n",
    "# save final model\n",
    "dt.save(f\"dt_final_model_{datetime.now().strftime('%Y%m%d-%H%M%S')}.d3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparam optimizaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from d3rlpy.algos import DiscreteDecisionTransformerConfig\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "import utils\n",
    "\n",
    "# Define the sweep configuration for Bayesian optimization\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'Cumulative Reward.Test_unreachable',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    # 'early_terminate': {\n",
    "    #     'type': 'hyperband',\n",
    "    #     'min_iter': 10\n",
    "    # },\n",
    "    'parameters': {\n",
    "        'batch_size': {\n",
    "            'values': [16, 32, 64, 128]\n",
    "        },\n",
    "        'gamma': {\n",
    "            'values': [0.95, 0.98, 0.99, 0.999]\n",
    "        },\n",
    "        'context_size': {   # TODO what is context_size?\n",
    "            'values': [5, 10, 15, 20, 25, 30]\n",
    "        },\n",
    "        'max_timestep': {   # TODO what is max_timestep?\n",
    "            'values': [50, 100, 300, 700, 1000]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'values': [1e-2, 1e-3, 1e-4, 6e-4, 1e-5]\n",
    "        },\n",
    "        'num_heads': {\n",
    "            'values': [3, 6, 8, 10]\n",
    "        },\n",
    "        'num_layers': {\n",
    "            'values': [2, 4, 6, 8]\n",
    "        },\n",
    "        'attn_dropout': {\n",
    "            'values': [0.05, 0.1, 0.15, 0.2, 0.25]\n",
    "        },\n",
    "        'resid_dropout': {\n",
    "            'values': [0.05, 0.1, 0.15, 0.2, 0.25]\n",
    "        },\n",
    "        'embed_dropout': {\n",
    "            'values': [0.05, 0.1, 0.15, 0.2, 0.25]\n",
    "        },\n",
    "        'warmup_tokens': {\n",
    "            'values': [512, 1024, 4096, 10240]\n",
    "        },\n",
    "        'n_steps_per_epoch': {\n",
    "            'values': [50, 100, 250, 500, 1000, 2000]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "n_total_steps = 10000\n",
    "\n",
    "\n",
    "train_env = utils.ObservationFlattenerWrapper(train_env)\n",
    "test_env_reachable = utils.ObservationFlattenerWrapper(test_env_reachable)\n",
    "test_env_unreachable = utils.ObservationFlattenerWrapper(test_env_unreachable)\n",
    "\n",
    "# Define the training function\n",
    "def train(config=None):\n",
    "    run_name = f\"DT_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    # Initialize wandb\n",
    "    with wandb.init(config=config, name=run_name):\n",
    "        # Retrieve hyperparameters\n",
    "        config = wandb.config\n",
    "        \n",
    "        # Initialize Decision Transformer with hyperparameters\n",
    "        dt_config = DiscreteDecisionTransformerConfig(\n",
    "            learning_rate=config.learning_rate,\n",
    "            batch_size=config.batch_size,\n",
    "            num_layers=config.num_layers,\n",
    "            num_heads=config.num_heads,\n",
    "            context_size=config.context_size,\n",
    "            attn_dropout=config.attn_dropout,\n",
    "            resid_dropout=config.resid_dropout,\n",
    "            embed_dropout=config.embed_dropout,\n",
    "            max_timestep=config.max_timestep,\n",
    "            warmup_tokens=config.warmup_tokens,\n",
    "            gamma=config.gamma\n",
    "        )\n",
    "        dt = dt_config.create(device=device)\n",
    "\n",
    "        \n",
    "        n_steps_per_epoch = config.n_steps_per_epoch\n",
    "        n_epochs = n_total_steps // n_steps_per_epoch\n",
    "\n",
    "        total_steps = 0\n",
    "        for epoch in tqdm(range(n_epochs)):\n",
    "            # Train the model\n",
    "            dt.fit(train_dataset, n_steps=n_steps_per_epoch, n_steps_per_epoch=n_steps_per_epoch, show_progress=False, save_interval=10000000)    # Don't save here\n",
    "\n",
    "            # Evaluate the model on training and test environments\n",
    "            train_eval_score, train_num_steps = eval_model(dt.as_stateful_wrapper(target_return=len(train_config[\"topologies\"]), action_sampler=None),\n",
    "                                                                    train_env,\n",
    "                                                                    len(train_config[\"topologies\"])\n",
    "                                                                    )\n",
    "            test_reachable_eval_score, test_reachable_num_steps = eval_model(dt.as_stateful_wrapper(target_return=len(train_config[\"topologies\"]), action_sampler=None),\n",
    "                                                                    test_env_reachable,\n",
    "                                                                    len(test_config_reachable[\"topologies\"])\n",
    "                                                                    )\n",
    "            test_unreachable_eval_score, test_unreachable_num_steps = eval_model(dt.as_stateful_wrapper(target_return=len(train_config[\"topologies\"]), action_sampler=None),\n",
    "                                                                    test_env_unreachable,\n",
    "                                                                    len(test_config_unreachable[\"topologies\"])\n",
    "                                                                    )\n",
    "\n",
    "            wandb.log({\"Cumulative Reward\": {\"Train\": train_eval_score, \"Test_reachable\": test_reachable_eval_score, \"Test_unreachable\": test_unreachable_eval_score}}, step=(epoch+1) * n_steps_per_epoch)\n",
    "            wandb.log({\"Total Number of steps taken\": {\"Train\": train_num_steps, \"Test_reachable\": test_reachable_num_steps, \"Test_unreachable\": test_unreachable_num_steps}}, step=(epoch+1) * n_steps_per_epoch)\n",
    "\n",
    "        # Save the final model, with config\n",
    "        dt.save(f\"run_{run_name}.d3\")\n",
    "        config_dict = config.as_dict()\n",
    "        config_dict[\"run_name\"] = run_name\n",
    "        # save as txt\n",
    "        with open(f\"run_{run_name}.txt\", \"w\") as f:\n",
    "            f.write(str(config_dict))\n",
    "        \n",
    "        \n",
    "\n",
    "# Initialize the sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"DT_hyperparameter_tuning\")\n",
    "\n",
    "# Run the sweep\n",
    "wandb.agent(sweep_id, function=train, count=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reachable test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-16 12:56.43 [info     ] dataset info                   dataset_info=DatasetInfo(observation_signature=Signature(dtype=[dtype('uint8')], shape=[(324,)]), action_signature=Signature(dtype=[dtype('int64')], shape=[(1,)]), reward_signature=Signature(dtype=[dtype('int32')], shape=[(1,)]), action_space=<ActionSpace.DISCRETE: 2>, action_size=3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:gymypgo5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DiscreteFQE_20240516125604</strong> at: <a href='https://wandb.ai/gold-ai/DT/runs/gymypgo5' target=\"_blank\">https://wandb.ai/gold-ai/DT/runs/gymypgo5</a><br/> View project at: <a href='https://wandb.ai/gold-ai/DT' target=\"_blank\">https://wandb.ai/gold-ai/DT</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240516_125604-gymypgo5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:gymypgo5). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\DT\\wandb\\run-20240516_125643-e4s9ea8t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gold-ai/DT/runs/e4s9ea8t' target=\"_blank\">DiscreteFQE_20240516125643</a></strong> to <a href='https://wandb.ai/gold-ai/DT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gold-ai/DT' target=\"_blank\">https://wandb.ai/gold-ai/DT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gold-ai/DT/runs/e4s9ea8t' target=\"_blank\">https://wandb.ai/gold-ai/DT/runs/e4s9ea8t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-16 12:56.50 [debug    ] Building models...            \n",
      "here\n",
      "2024-05-16 12:56.50 [info     ] dataset info                   dataset_info=DatasetInfo(observation_signature=Signature(dtype=[dtype('uint8')], shape=[(324,)]), action_signature=Signature(dtype=[dtype('int64')], shape=[(1,)]), reward_signature=Signature(dtype=[dtype('int32')], shape=[(1,)]), action_space=<ActionSpace.DISCRETE: 2>, action_size=3)\n",
      "2024-05-16 12:56.50 [info     ] Directory is created at d3rlpy_logs\\DiscreteDecisionTransformer_20240516125650\n",
      "2024-05-16 12:56.50 [warning  ] Skip building models since they're already built.\n",
      "2024-05-16 12:56.50 [info     ] Parameters                     params={'observation_shape': [324], 'action_size': 3, 'config': {'type': 'discrete_decision_transformer', 'params': {'batch_size': 128, 'gamma': 0.99, 'observation_scaler': {'type': 'none', 'params': {}}, 'action_scaler': {'type': 'none', 'params': {}}, 'reward_scaler': {'type': 'none', 'params': {}}, 'context_size': 20, 'max_timestep': 1000, 'learning_rate': 0.0006, 'encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'optim_factory': {'type': 'adam', 'params': {'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}}, 'num_heads': 8, 'num_layers': 6, 'attn_dropout': 0.1, 'resid_dropout': 0.1, 'embed_dropout': 0.1, 'activation_type': 'gelu', 'embed_activation_type': 'tanh', 'position_encoding_type': <PositionEncodingType.GLOBAL: 'global'>, 'warmup_tokens': 10240, 'final_tokens': 30000000, 'clip_grad_norm': 1.0, 'compile': False}}}\n",
      "2024-05-16 12:58.14 [info     ] dataset info                   dataset_info=DatasetInfo(observation_signature=Signature(dtype=[dtype('uint8')], shape=[(324,)]), action_signature=Signature(dtype=[dtype('int64')], shape=[(1,)]), reward_signature=Signature(dtype=[dtype('int32')], shape=[(1,)]), action_space=<ActionSpace.DISCRETE: 2>, action_size=3)\n",
      "2024-05-16 12:58.14 [info     ] Directory is created at d3rlpy_logs\\DiscreteDecisionTransformer_20240516125814\n",
      "2024-05-16 12:58.14 [warning  ] Skip building models since they're already built.\n",
      "2024-05-16 12:58.14 [info     ] Parameters                     params={'observation_shape': [324], 'action_size': 3, 'config': {'type': 'discrete_decision_transformer', 'params': {'batch_size': 128, 'gamma': 0.99, 'observation_scaler': {'type': 'none', 'params': {}}, 'action_scaler': {'type': 'none', 'params': {}}, 'reward_scaler': {'type': 'none', 'params': {}}, 'context_size': 20, 'max_timestep': 1000, 'learning_rate': 0.0006, 'encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'optim_factory': {'type': 'adam', 'params': {'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}}, 'num_heads': 8, 'num_layers': 6, 'attn_dropout': 0.1, 'resid_dropout': 0.1, 'embed_dropout': 0.1, 'activation_type': 'gelu', 'embed_activation_type': 'tanh', 'position_encoding_type': <PositionEncodingType.GLOBAL: 'global'>, 'warmup_tokens': 10240, 'final_tokens': 30000000, 'clip_grad_norm': 1.0, 'compile': False}}}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 15\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# test with offline dataset\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43moffline_policy_evaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mn_steps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetric_td_error\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mTDErrorEvaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetric_discrete_action_match\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mDiscreteActionMatchEvaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mlogger_adapter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWanDBAdapterFactory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# reset history\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\d3rlpy\\algos\\qlearning\\base.py:409\u001b[0m, in \u001b[0;36mQLearningAlgoBase.fit\u001b[1;34m(self, dataset, n_steps, n_steps_per_epoch, experiment_name, with_timestamp, logging_steps, logging_strategy, logger_adapter, show_progress, save_interval, evaluators, callback, epoch_callback, enable_ddp)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Trains with given dataset.\u001b[39;00m\n\u001b[0;32m    378\u001b[0m \n\u001b[0;32m    379\u001b[0m \u001b[38;5;124;03m.. code-block:: python\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;124;03m    List of result tuples (epoch, metrics) per epoch.\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfitter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_steps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_timestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_timestamp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger_adapter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_adapter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepoch_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_ddp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_ddp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\d3rlpy\\algos\\qlearning\\base.py:507\u001b[0m, in \u001b[0;36mQLearningAlgoBase.fitter\u001b[1;34m(self, dataset, n_steps, n_steps_per_epoch, logging_steps, logging_strategy, experiment_name, with_timestamp, logger_adapter, show_progress, save_interval, evaluators, callback, epoch_callback, enable_ddp)\u001b[0m\n\u001b[0;32m    506\u001b[0m     observation_shape \u001b[38;5;241m=\u001b[39m observation_shape[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m LOG\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModels have been built.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\d3rlpy\\base.py:311\u001b[0m, in \u001b[0;36mLearnableBase.create_impl\u001b[1;34m(self, observation_shape, action_size)\u001b[0m\n\u001b[0;32m    310\u001b[0m     LOG\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameters will be reinitialized.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_create_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\d3rlpy\\ope\\fqe.py:222\u001b[0m, in \u001b[0;36mDiscreteFQE.inner_create_impl\u001b[1;34m(self, observation_shape, action_size)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_create_impl\u001b[39m(\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m, observation_shape: Shape, action_size: \u001b[38;5;28mint\u001b[39m\n\u001b[0;32m    221\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_algo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe target algorithm is not initialized.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    224\u001b[0m     q_funcs, q_func_forwarder \u001b[38;5;241m=\u001b[39m create_discrete_q_function(\n\u001b[0;32m    225\u001b[0m         observation_shape,\n\u001b[0;32m    226\u001b[0m         action_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device,\n\u001b[0;32m    231\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'StatefulTransformerWrapper' object has no attribute 'impl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 46\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     observation, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m test_env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     48\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\d3rlpy\\algos\\transformer\\base.py:167\u001b[0m, in \u001b[0;36mStatefulTransformerWrapper.predict\u001b[1;34m(self, x, reward)\u001b[0m\n\u001b[0;32m    155\u001b[0m     numpy_observations \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    156\u001b[0m         np\u001b[38;5;241m.\u001b[39marray([o[i] \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_observations])\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x))\n\u001b[0;32m    158\u001b[0m     ]\n\u001b[0;32m    160\u001b[0m inpt \u001b[38;5;241m=\u001b[39m TransformerInput(\n\u001b[0;32m    161\u001b[0m     observations\u001b[38;5;241m=\u001b[39mnumpy_observations,\n\u001b[0;32m    162\u001b[0m     actions\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actions),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m     timesteps\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timesteps),\n\u001b[0;32m    166\u001b[0m )\n\u001b[1;32m--> 167\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_sampler(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_algo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actions[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m action\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actions\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_pad_action())\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\d3rlpy\\algos\\transformer\\base.py:227\u001b[0m, in \u001b[0;36mTransformerAlgoBase.predict\u001b[1;34m(self, inpt)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    219\u001b[0m     torch_inpt \u001b[38;5;241m=\u001b[39m TorchTransformerInput\u001b[38;5;241m.\u001b[39mfrom_numpy(\n\u001b[0;32m    220\u001b[0m         inpt\u001b[38;5;241m=\u001b[39minpt,\n\u001b[0;32m    221\u001b[0m         context_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mcontext_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    225\u001b[0m         reward_scaler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mreward_scaler,\n\u001b[0;32m    226\u001b[0m     )\n\u001b[1;32m--> 227\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_impl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch_inpt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39maction_scaler:\n\u001b[0;32m    230\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39maction_scaler\u001b[38;5;241m.\u001b[39mreverse_transform(action)\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\d3rlpy\\torch_utility.py:355\u001b[0m, in \u001b[0;36meval_api.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules, Modules)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mset_eval()\n\u001b[1;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\d3rlpy\\algos\\transformer\\base.py:45\u001b[0m, in \u001b[0;36mTransformerAlgoImplBase.predict\u001b[1;34m(self, inpt)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;129m@eval_api\u001b[39m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, inpt: TorchTransformerInput) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\d3rlpy\\algos\\transformer\\torch\\decision_transformer_impl.py:131\u001b[0m, in \u001b[0;36mDiscreteDecisionTransformerImpl.inner_predict\u001b[1;34m(self, inpt)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, inpt: TorchTransformerInput) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;66;03m# (1, T, A)\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m     _, logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43minpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturns_to_go\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimesteps\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;66;03m# (1, T, A) -> (A,)\u001b[39;00m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\d3rlpy\\models\\torch\\transformers.py:430\u001b[0m, in \u001b[0;36mDiscreteDecisionTransformer.forward\u001b[1;34m(self, x, action, return_to_go, timesteps)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m    428\u001b[0m     h \u001b[38;5;241m=\u001b[39m h[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m--> 430\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gpt2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;66;03m# use state embeddings as input\u001b[39;00m\n\u001b[0;32m    433\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output(h[:, \u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m3\u001b[39m, :])\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\d3rlpy\\models\\torch\\transformers.py:251\u001b[0m, in \u001b[0;36mGPT2.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    250\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dropout(x)\n\u001b[1;32m--> 251\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layer_norm(h)\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m h\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\d3rlpy\\models\\torch\\transformers.py:155\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    153\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attention(norm_x)\n\u001b[0;32m    154\u001b[0m norm_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layer_norm2(x)\n\u001b[1;32m--> 155\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\d3rlpy\\models\\torch\\transformers.py:111\u001b[0m, in \u001b[0;36mMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    110\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_l1(x))\n\u001b[1;32m--> 111\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_l2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m h\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test the trained model with offline policy evaluator on reachable test set\n",
    "\n",
    "imgs = []\n",
    "# # wrap as stateful actor for interaction\n",
    "try:\n",
    "    actor = dt.as_stateful_wrapper(target_return=0)\n",
    "\n",
    "    offline_policy_evaluator = DiscreteFQE(algo=actor, config=FQEConfig())\n",
    "    # test with offline dataset\n",
    "    offline_policy_evaluator.fit(test_dataset_reachable,\n",
    "                                n_steps=2,\n",
    "                                n_steps_per_epoch=1,\n",
    "                                evaluators={\"metric_td_error\": TDErrorEvaluator(),\n",
    "                                            \"metric_discrete_action_match\": DiscreteActionMatchEvaluator()},\n",
    "                                logger_adapter=WanDBAdapterFactory(project=\"DT\")\n",
    "                        )\n",
    "\n",
    "    # reset history\n",
    "    actor.reset()\n",
    "except:\n",
    "    print(\"here\")\n",
    "    # do hardcoded test\n",
    "    # Test model explicitly\n",
    "    test_env_reachable = utils.ObservationFlattenerWrapper(test_env_reachable)\n",
    "    rewards = []\n",
    "    # wrap as stateful actor for interaction\n",
    "    actor = dt.as_stateful_wrapper(target_return=0)\n",
    "    # explicitly evaluate the model\n",
    "    for _ in range(len(test_config_reachable[\"topologies\"])):\n",
    "        img = test_env_reachable.render()\n",
    "        imgs.append(img)\n",
    "        observation, _ = test_env_reachable.reset()\n",
    "        reward = 0.0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = actor.predict(observation, reward)\n",
    "            observation, reward, terminated, truncated, _ = test_env_reachable.step(action)\n",
    "            done = terminated or truncated\n",
    "            rewards.append(reward)\n",
    "\n",
    "    actor.reset()\n",
    "\n",
    "    print(\"Cumulative Reward: \", np.sum(rewards))\n",
    "    imageio.mimsave('test_rendered_episode.gif', [np.array(img) for i, img in enumerate(imgs) if i%1 == 0], duration=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unreachable test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model with offline policy evaluator on unreachable test set\n",
    "\n",
    "imgs = []\n",
    "# # wrap as stateful actor for interaction\n",
    "try:\n",
    "    actor = dt.as_stateful_wrapper(target_return=0)\n",
    "\n",
    "    offline_policy_evaluator = DiscreteFQE(algo=actor, config=FQEConfig())\n",
    "    # test with offline dataset\n",
    "    offline_policy_evaluator.fit(test_dataset_unreachable,\n",
    "                                n_steps=2,\n",
    "                                n_steps_per_epoch=1,\n",
    "                                evaluators={\"metric_td_error\": TDErrorEvaluator(),\n",
    "                                            \"metric_discrete_action_match\": DiscreteActionMatchEvaluator()},\n",
    "                                logger_adapter=WanDBAdapterFactory(project=\"DT\")\n",
    "                        )\n",
    "\n",
    "    # reset history\n",
    "    actor.reset()\n",
    "except:\n",
    "    print(\"here\")\n",
    "    # do hardcoded test\n",
    "    # Test model explicitly\n",
    "    test_env_unreachable = utils.ObservationFlattenerWrapper(test_env_unreachable)\n",
    "    rewards = []\n",
    "    # wrap as stateful actor for interaction\n",
    "    actor = dt.as_stateful_wrapper(target_return=0)\n",
    "    # explicitly evaluate the model\n",
    "    for _ in range(len(test_config_unreachable[\"topologies\"])):\n",
    "        img = test_env_unreachable.render()\n",
    "        imgs.append(img)\n",
    "        observation, _ = test_env_unreachable.reset()\n",
    "        reward = 0.0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = actor.predict(observation, reward)\n",
    "            observation, reward, terminated, truncated, _ = test_env_unreachable.step(action)\n",
    "            done = terminated or truncated\n",
    "            rewards.append(reward)\n",
    "\n",
    "    actor.reset()\n",
    "\n",
    "    print(\"Cumulative Reward: \", np.sum(rewards))\n",
    "    imageio.mimsave('test_unreachable_rendered_episode.gif', [np.array(img) for i, img in enumerate(imgs) if i%1 == 0], duration=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d3rlpy.datasets import get_cartpole # CartPole-v1 dataset\n",
    "dataset, env = get_cartpole()\n",
    "from d3rlpy.algos import DQNConfig\n",
    "\n",
    "# if you don't use GPU, set device=None instead.\n",
    "dqn = DQNConfig().create(device=device)\n",
    "\n",
    "# initialize neural networks with the given observation shape and action size.\n",
    "# this is not necessary when you directly call fit or fit_online method.\n",
    "dqn.build_with_dataset(dataset)\n",
    "from d3rlpy.metrics import TDErrorEvaluator\n",
    "\n",
    "# calculate metrics with training dataset\n",
    "td_error_evaluator = TDErrorEvaluator(episodes=dataset.episodes)\n",
    "\n",
    "\n",
    "from d3rlpy.metrics import EnvironmentEvaluator\n",
    "\n",
    "# set environment in scorer function\n",
    "env_evaluator = EnvironmentEvaluator(env)\n",
    "\n",
    "# evaluate algorithm on the environment\n",
    "rewards = env_evaluator(dqn, dataset=None)\n",
    "\n",
    "dqn.fit(\n",
    "    dataset,\n",
    "    n_steps=10,\n",
    "    n_steps_per_epoch=2,\n",
    "    evaluators={\n",
    "        'td_error': td_error_evaluator,\n",
    "        'reward': env_evaluator,\n",
    "        \"metric_discrete_action_match\": DiscreteActionMatchEvaluator()\n",
    "    },\n",
    "    logger_adapter=WanDBAdapterFactory(project=\"random_test_runs\")\n",
    ")\n",
    "\n",
    "import d3rlpy\n",
    "\n",
    "# prepare the trained algorithm\n",
    "\n",
    "# dataset to evaluate with\n",
    "dataset, env = get_cartpole()\n",
    "\n",
    "# off-policy evaluation algorithm\n",
    "fqe = d3rlpy.ope.DiscreteFQE(algo=dqn, config=d3rlpy.ope.FQEConfig(), device=device)\n",
    "\n",
    "# train estimators to evaluate the trained policy\n",
    "fqe.fit(\n",
    "   dataset,\n",
    "   n_steps=10,\n",
    "   n_steps_per_epoch=2,\n",
    "   evaluators={\n",
    "        'td_error': td_error_evaluator,\n",
    "        'environment': env_evaluator,\n",
    "        \"metric_discrete_action_match\": DiscreteActionMatchEvaluator()\n",
    "    },\n",
    "   logger_adapter=WanDBAdapterFactory(project=\"random_test_runs\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-16 08:58.11 [info     ] Signatures have been automatically determined. action_signature=Signature(dtype=[dtype('int32')], shape=[(1,)]) observation_signature=Signature(dtype=[dtype('float32')], shape=[(4,)]) reward_signature=Signature(dtype=[dtype('float32')], shape=[(1,)])\n",
      "2024-05-16 08:58.11 [info     ] Action-space has been automatically determined. action_space=<ActionSpace.DISCRETE: 2>\n",
      "2024-05-16 08:58.11 [info     ] Action size has been automatically determined. action_size=2\n",
      "2024-05-16 08:58.11 [info     ] dataset info                   dataset_info=DatasetInfo(observation_signature=Signature(dtype=[dtype('float32')], shape=[(4,)]), action_signature=Signature(dtype=[dtype('int32')], shape=[(1,)]), reward_signature=Signature(dtype=[dtype('float32')], shape=[(1,)]), action_space=<ActionSpace.DISCRETE: 2>, action_size=2)\n",
      "2024-05-16 08:58.11 [info     ] Directory is created at d3rlpy_logs\\DiscreteDecisionTransformer_20240516085811\n",
      "2024-05-16 08:58.11 [debug    ] Building models...            \n",
      "2024-05-16 08:58.11 [debug    ] Models have been built.       \n",
      "2024-05-16 08:58.11 [info     ] Parameters                     params={'observation_shape': [4], 'action_size': 2, 'config': {'type': 'discrete_decision_transformer', 'params': {'batch_size': 128, 'gamma': 0.99, 'observation_scaler': {'type': 'none', 'params': {}}, 'action_scaler': {'type': 'none', 'params': {}}, 'reward_scaler': {'type': 'none', 'params': {}}, 'context_size': 20, 'max_timestep': 1000, 'learning_rate': 0.0006, 'encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'optim_factory': {'type': 'adam', 'params': {'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}}, 'num_heads': 8, 'num_layers': 6, 'attn_dropout': 0.1, 'resid_dropout': 0.1, 'embed_dropout': 0.1, 'activation_type': 'gelu', 'embed_activation_type': 'tanh', 'position_encoding_type': <PositionEncodingType.GLOBAL: 'global'>, 'warmup_tokens': 10240, 'final_tokens': 30000000, 'clip_grad_norm': 1.0, 'compile': False}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 2/2 [00:06<00:00,  3.07s/it, loss=0.733, learning_rate=0.000131]\n",
      "c:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-16 08:58.20 [info     ] DiscreteDecisionTransformer_20240516085811: epoch=1 step=2 epoch=1 metrics={'time_sample_batch': 0.014497637748718262, 'time_algorithm_update': 3.049818992614746, 'loss': 1.3235219717025757, 'learning_rate': 0.00019494140625, 'time_step': 3.0648165941238403, 'environment': 9.4} step=2\n",
      "2024-05-16 08:58.21 [info     ] Model parameters are saved to d3rlpy_logs\\DiscreteDecisionTransformer_20240516085811\\model_2.d3\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import d3rlpy\n",
    "\n",
    "dataset, env = d3rlpy.datasets.get_cartpole()\n",
    "\n",
    "dt = d3rlpy.algos.DiscreteDecisionTransformerConfig().create(device=device)\n",
    "\n",
    "# offline training\n",
    "print(dt.fit(\n",
    "   dataset,\n",
    "   n_steps=2,\n",
    "   n_steps_per_epoch=2,\n",
    "   eval_env=env,\n",
    "   eval_target_return=0,  # specify target environment return\n",
    "))\n",
    "\n",
    "# wrap as stateful actor for interaction\n",
    "actor = dt.as_stateful_wrapper(target_return=0)\n",
    "\n",
    "# interaction\n",
    "observation, reward = env.reset(), 0.0\n",
    "observation = observation[0]\n",
    "while True:\n",
    "    action = actor.predict(observation, reward)\n",
    "    observation, reward, done, truncated, _ = env.step(action)\n",
    "    if done or truncated:\n",
    "        break\n",
    "\n",
    "# reset history\n",
    "actor.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
