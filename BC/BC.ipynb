{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.random.seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01md3rlpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgos\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DiscreteBCConfig\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseed\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp_seed\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanual_seed\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtorch_seed\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseed\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mrandom_seed\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy.random.seed'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import dill\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "from four_room.env import FourRoomsEnv\n",
    "from four_room.wrappers import gym_wrapper\n",
    "from four_room.shortest_path import find_all_action_values\n",
    "from four_room.utils import obs_to_state\n",
    "from four_room_extensions import fourrooms_dataset_gen\n",
    "from d3rlpy.algos import DiscreteBCConfig\n",
    "from d3rlpy.metrics import EnvironmentEvaluator, TDErrorEvaluator, DiscreteActionMatchEvaluator, evaluate_transformer_with_environment\n",
    "from d3rlpy.datasets import MDPDataset\n",
    "from d3rlpy.logging import WanDBAdapterFactory\n",
    "from d3rlpy.ope import FQEConfig, DiscreteFQE\n",
    "from d3rlpy import load_learnable\n",
    "import wandb\n",
    "import utils\n",
    "from datetime import datetime\n",
    "import imageio\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from utils import get_DQN_checkpoints, create_env\n",
    "import pickle\n",
    "from four_room_extensions.fourrooms_dataset_gen import get_mixed_policy_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "batch_size = 100 # default value from library\n",
    "learning_rate = 0.001 # default value from library\n",
    "\n",
    "n_epochs = 100\n",
    "n_steps_per_epoch = 50  # Evaluation done after thin many steps, but we can change that logic\n",
    "\n",
    "train_config_path = 'train'\n",
    "reachable_test_config_path = 'test_100'\n",
    "unreachable_test_config_path = 'test_0'\n",
    "render = False\n",
    "wandb_project_name = \"BC\"\n",
    "device = True if torch.cuda.is_available() else None\n",
    "\n",
    "\n",
    "## if using stored mixed data, use these\n",
    "mixed_data_file = \"\"\n",
    "wandb_run_name = f\"{mixed_data_file}-BC_mixed\"\n",
    "DQN_mixed_data_path = os.path.join(\"/kaggle/working/offline_multi_task_rl\", \"datasets\", \"dataset_from_models_\", mixed_data_file)\n",
    "\n",
    "## if simulating mixed_data, use these\n",
    "# best_policy = True\n",
    "# DQN_models_path = os.path.join(\"/kaggle/working/offline_multi_task_rl\", \"four_room_extensions\", \"DQN_models\", \"performance_per_model.txt\")\n",
    "# episode_length = [0, 25, 50, 75, 100]\n",
    "# wandb_run_name = f\"{episode_length}-best_policy-100epochs-50stepsPerEpoch\"\n",
    "\n",
    "\n",
    "wandb_config = {\n",
    "                \"n_epochs\": n_epochs,\n",
    "                \"n_steps_per_epoch\": n_steps_per_epoch,\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = fourrooms_dataset_gen.get_config(train_config_path)\n",
    "train_dataset, train_env, tasks_finished, tasks_failed = fourrooms_dataset_gen.get_expert_dataset_from_config(train_config, render=render, render_name=\"DT_train_expert\")\n",
    "\n",
    "train_dataset = MDPDataset(\n",
    "    observations=train_dataset.get(\"observations\"),\n",
    "    actions=train_dataset.get(\"actions\"),\n",
    "    rewards=train_dataset.get(\"rewards\"),\n",
    "    terminals=train_dataset.get(\"terminals\"),\n",
    ")\n",
    "\n",
    "test_config_reachable = fourrooms_dataset_gen.get_config(reachable_test_config_path)\n",
    "test_dataset_reachable, test_env_reachable, tasks_finished, tasks_failed = fourrooms_dataset_gen.get_expert_dataset_from_config(test_config_reachable, render=render, render_name=\"DT_test_expert_reachable\")\n",
    "\n",
    "test_dataset_reachable = MDPDataset(\n",
    "    observations=test_dataset_reachable.get(\"observations\"),\n",
    "    actions=test_dataset_reachable.get(\"actions\"),\n",
    "    rewards=test_dataset_reachable.get(\"rewards\"),\n",
    "    terminals=test_dataset_reachable.get(\"terminals\"),\n",
    ")\n",
    "\n",
    "test_config_unreachable = fourrooms_dataset_gen.get_config(unreachable_test_config_path)\n",
    "test_dataset_unreachable, test_env_unreachable, tasks_finished, tasks_failed = fourrooms_dataset_gen.get_expert_dataset_from_config(test_config_unreachable, render=render, render_name=\"DT_test_expert_unreachable\")\n",
    "\n",
    "test_dataset_unreachable = MDPDataset(\n",
    "    observations=test_dataset_unreachable.get(\"observations\"),\n",
    "    actions=test_dataset_unreachable.get(\"actions\"),\n",
    "    rewards=test_dataset_unreachable.get(\"rewards\"),\n",
    "    terminals=test_dataset_unreachable.get(\"terminals\"),\n",
    ")\n",
    "\n",
    "train_env = create_env(train_config)\n",
    "# checkpoints = get_DQN_checkpoints(DQN_models_path, episode_length, best_policy=best_policy)\n",
    "# mixed_dataset, finished, failed = get_mixed_policy_dataset(train_config, train_env, checkpoints)\n",
    "with open(DQN_mixed_data_path, 'rb') as f:\n",
    "            mixed_dataset = pickle.load(f)\n",
    "\n",
    "mixed_dataset = MDPDataset(\n",
    "    observations=mixed_dataset.get(\"observations\"),\n",
    "    actions=mixed_dataset.get(\"actions\"),\n",
    "    rewards=mixed_dataset.get(\"rewards\"),\n",
    "    terminals=mixed_dataset.get(\"terminals\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_saver_d3rlpy_callback(algo, epoch, total_step, n_epochs, n_steps_per_epoch, title_addition = \"\"):\n",
    "    \"\"\"\n",
    "    Callback to save the model at the end of each epoch\n",
    "\n",
    "    Args:\n",
    "        algo: The algorithm object\n",
    "        epoch: The current epoch\n",
    "        total_step: The total number of steps taken so far\n",
    "        n_epochs: The total number of epochs\n",
    "        n_steps_per_epoch: The number of steps in each epoch\n",
    "    # \"\"\"\n",
    "    algo.save(f\"dt_{title_addition}_model_at_step_{total_step}_{datetime.now().strftime('%Y%m%d-%H%M%S')}.d3\")\n",
    "        \n",
    "def eval_model(policy, env, n_episodes):\n",
    "    total_reward = 0\n",
    "    n_steps_taken = 0\n",
    "    for _ in range(n_episodes):\n",
    "        policy.reset()\n",
    "        observation, reward = env.reset(seed=seed)[0], 0.0\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            # take action\n",
    "            n_steps_taken += 1\n",
    "            action = policy.predict(observation, reward)\n",
    "\n",
    "            observation, _reward, terminated, truncated, _ = env.step(action)\n",
    "            reward = float(_reward)\n",
    "            total_reward += reward\n",
    "            done = terminated or truncated\n",
    "\n",
    "    return total_reward / n_episodes, n_steps_taken / n_episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BC = DiscreteBCConfig(batch_size=batch_size, learning_rate=learning_rate).create(device=device)\n",
    "\n",
    "model_saver_d3rlpy_callback_partial = partial(model_saver_d3rlpy_callback, n_epochs=n_epochs, n_steps_per_epoch=n_steps_per_epoch, title_addition=wandb_run_name)\n",
    "\n",
    "train_env = utils.ObservationFlattenerWrapper(train_env)\n",
    "test_env_reachable = utils.ObservationFlattenerWrapper(test_env_reachable)\n",
    "test_env_unreachable = utils.ObservationFlattenerWrapper(test_env_unreachable)\n",
    "\n",
    "with wandb.init(project=wandb_project_name, name=wandb_run_name, config=wandb_config):\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        BC.fit(mixed_dataset, n_steps=n_steps_per_epoch, n_steps_per_epoch=n_steps_per_epoch, epoch_callback=model_saver_d3rlpy_callback_partial, show_progress=False, save_interval=1000)\n",
    "\n",
    "        train_eval_score, train_num_steps = eval_model(BC.as_stateful_wrapper(target_return=1, action_sampler=None),\n",
    "                                                                 train_env,\n",
    "                                                                 len(train_config[\"topologies\"])\n",
    "                                                                 )\n",
    "        test_reachable_eval_score, test_reachable_num_steps = eval_model(BC.as_stateful_wrapper(target_return=1, action_sampler=None),\n",
    "                                                               test_env_reachable,\n",
    "                                                               len(test_config_reachable[\"topologies\"])\n",
    "                                                               )\n",
    "        test_unreachable_eval_score, test_unreachable_num_steps = eval_model(BC.as_stateful_wrapper(target_return=1, action_sampler=None),\n",
    "                                                                test_env_unreachable,\n",
    "                                                                len(test_config_unreachable[\"topologies\"])\n",
    "                                                                )\n",
    "        \n",
    "        wandb.log({\"Cumulative Reward\": {\"Train\": train_eval_score, \"Test_reachable\": test_reachable_eval_score, \"Test_unreachable\": test_unreachable_eval_score}}, step=(epoch+1) * n_steps_per_epoch)\n",
    "        wandb.log({\"Number of steps taken\": {\"Train\": train_num_steps, \"Test_reachable\": test_reachable_num_steps, \"Test_unreachable\": test_unreachable_num_steps}}, step=(epoch+1) * n_steps_per_epoch)\n",
    "    \n",
    "\n",
    "# save final model\n",
    "BC.save(f\"dt_final_model_{datetime.now().strftime('%Y%m%d-%H%M%S')}.d3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
