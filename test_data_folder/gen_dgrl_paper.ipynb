{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [01:00<14:07, 60.55s/it]2024-05-30 16:42:11,776 [torchrl][INFO] Downloading dataset file fruitbot.tar.xz (https://dl.fbaipublicfiles.com/DGRL/Procgen/Datasets/Compressed/1M/level_200/expert/fruitbot.tar.xz) to /tmp/tmp_h_j7536/_in_progress/fruitbot.tar.xz.\n",
      "2024-05-30 16:42:12,038 [torchrl][INFO] Downloading dataset file heist.tar.xz (https://dl.fbaipublicfiles.com/DGRL/Procgen/Datasets/Compressed/1M/level_200/expert/heist.tar.xz) to /tmp/tmpc3z_8c0p/_in_progress/heist.tar.xz.\n",
      "  7%|▋         | 1/15 [01:05<15:14, 65.32s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "unable to resize file <caveflyer-1M_E/next/observation.memmap> to the right size: No space left on device (28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/c/Users/shaya/Documents/TU_projects/random/offline_multi_task_rl/test_data_folder/.venv_linux/lib/python3.11/site-packages/torchrl/data/datasets/gen_dgrl.py\", line 163, in __init__\n    storage = TensorStorage(TensorDict.load_memmap(self.data_path_root))\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/c/Users/shaya/Documents/TU_projects/random/offline_multi_task_rl/test_data_folder/.venv_linux/lib/python3.11/site-packages/tensordict/base.py\", line 2589, in load_memmap\n    return cls._load_memmap(prefix, metadata)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/c/Users/shaya/Documents/TU_projects/random/offline_multi_task_rl/test_data_folder/.venv_linux/lib/python3.11/site-packages/tensordict/_td.py\", line 2267, in _load_memmap\n    out.set(key, TensorDict.load_memmap(path))\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/c/Users/shaya/Documents/TU_projects/random/offline_multi_task_rl/test_data_folder/.venv_linux/lib/python3.11/site-packages/tensordict/base.py\", line 2589, in load_memmap\n    return cls._load_memmap(prefix, metadata)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/c/Users/shaya/Documents/TU_projects/random/offline_multi_task_rl/test_data_folder/.venv_linux/lib/python3.11/site-packages/tensordict/_td.py\", line 2254, in _load_memmap\n    MemoryMappedTensor.from_filename(\n  File \"/mnt/c/Users/shaya/Documents/TU_projects/random/offline_multi_task_rl/test_data_folder/.venv_linux/lib/python3.11/site-packages/tensordict/memmap.py\", line 463, in from_filename\n    tensor = torch.from_file(\n             ^^^^^^^^^^^^^^^^\nRuntimeError: unable to resize file <caveflyer-1M_E/next/observation.memmap> to the right size: No space left on device (28)\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m datasets \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00md\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-1M_E\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(\u001b[38;5;241m4\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m---> 31\u001b[0m      data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGenDGRLExperienceReplay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/shaya/Documents/TU_projects/random/offline_multi_task_rl/test_data_folder/.venv_linux/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[0;31mRuntimeError\u001b[0m: unable to resize file <caveflyer-1M_E/next/observation.memmap> to the right size: No space left on device (28)"
     ]
    }
   ],
   "source": [
    "from torchrl.data.datasets import GenDGRLExperienceReplay\n",
    "datasets = [\n",
    "     # \"bigfish\",\n",
    "          \"bossfight\",\n",
    "     \"caveflyer\",\n",
    "     \"chaser\",\n",
    "     \"climber\",\n",
    "     \"coinrun\",\n",
    "     \"dodgeball\",\n",
    "     \"fruitbot\",\n",
    "     \"heist\",\n",
    "     \"jumper\",\n",
    "     \"leaper\",\n",
    "     \"maze\",\n",
    "     \"miner\",\n",
    "     \"ninja\",\n",
    "     \"plunder\",\n",
    "     \"starpilot\",\n",
    "          ]\n",
    "# d = GenDGRLExperienceReplay(\"bigfish-1M_E\", batch_size=32, root=\"./\")\n",
    "\n",
    "\n",
    "# parallel download\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "datasets = [f\"{d}-1M_E\" for d in datasets]\n",
    "\n",
    "with Pool(4) as p:\n",
    "     data = list(tqdm(p.imap(partial(GenDGRLExperienceReplay, batch_size=32, root=\"./\"), datasets), total=len(datasets)))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will download 16 files from the following links for 1M_E: ['https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\bigfish.tar.xz', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\bossfight.tar.xz', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\caveflyer.tar.xz', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\chaser.tar.xz', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\climber.tar.xz', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\coinrun.tar.xz', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\dodgeball.tar.xz', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\fruitbot.tar.xz', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\heist.tar.xz', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\jumper.tar.xz', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\leaper.tar.xz', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\maze.tar.xz', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\miner.tar.xz', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\ninja.tar.xz', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\plunder.tar.xz', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\starpilot.tar.xz']\n",
      "['https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\bigfish.tar', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\bossfight.tar', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\caveflyer.tar', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\chaser.tar', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\climber.tar', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\coinrun.tar', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\dodgeball.tar', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\fruitbot.tar', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\heist.tar', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\jumper.tar', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\leaper.tar', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\maze.tar', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\miner.tar', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\ninja.tar', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\plunder.tar', 'https://dl.fbaipublicfiles.com/DGRL/1M/expert\\\\starpilot.tar']\n",
      "Downloading ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 50.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 16 dataset files ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Error while decompressing: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\shaya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\downloader.py\", line 223, in _unpack_category_file\n    decompressed_data = pylzma.decompress(f.read())\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: Error while decompressing: 1\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download_dataset\n\u001b[1;32m----> 4\u001b[0m \u001b[43mdownload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategory_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1M_E\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_data_folder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\downloader.py:114\u001b[0m, in \u001b[0;36mdownload_dataset\u001b[1;34m(category_name, download_folder, n_download_workers, n_extract_workers, clear_archives_after_unpacking, skip_downloaded_archives)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data_links)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dataset files ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mp\u001b[38;5;241m.\u001b[39mPool(processes\u001b[38;5;241m=\u001b[39mn_extract_workers) \u001b[38;5;28;01mas\u001b[39;00m extract_pool:\n\u001b[1;32m--> 114\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextract_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunctools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_unpack_category_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdownload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m                \u001b[49m\u001b[43mclear_archives_after_unpacking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_links\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mpass\u001b[39;49;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[1;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[1;31mTypeError\u001b[0m: Error while decompressing: 1"
     ]
    }
   ],
   "source": [
    "# from downloader import download_dataset\n",
    "\n",
    "\n",
    "# download_dataset(category_name=\"1M_E\", download_folder=\"test_data_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot load file containing pickled data when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m         episode[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrewards\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m episode[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrewards\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m episode\n\u001b[1;32m---> 11\u001b[0m episodes \u001b[38;5;241m=\u001b[39m \u001b[43mload_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgen_dgrl_data/caveflyer/caveflyer/caveflyer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# load pickle file\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgen_dgrl_data/caveflyer/caveflyer/caveflyer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m, in \u001b[0;36mload_episode\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_episode\u001b[39m(path) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 5\u001b[0m         episode \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m         episode \u001b[38;5;241m=\u001b[39m {k: episode[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m episode\u001b[38;5;241m.\u001b[39mkeys()}\n\u001b[0;32m      7\u001b[0m         episode[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservations\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m episode[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservations\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n",
      "File \u001b[1;32mc:\\Users\\shaya\\Documents\\TU_projects\\random\\offline_multi_task_rl\\.venv\\Lib\\site-packages\\numpy\\lib\\npyio.py:462\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n\u001b[1;32m--> 462\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot load file containing pickled data \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    463\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen allow_pickle=False\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mload(fid, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_kwargs)\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot load file containing pickled data when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "def load_episode(path) -> dict[str, np.ndarray]:\n",
    "    with open(path, \"rb\") as f:\n",
    "        episode = np.load(f)\n",
    "        episode = {k: episode[k] for k in episode.keys()}\n",
    "        episode['observations'] = episode['observations'].astype(np.uint8)\n",
    "        episode['rewards'] = episode['rewards'].astype(np.float)\n",
    "        return episode\n",
    "    \n",
    "episodes = load_episode(\"gen_dgrl_data/caveflyer\")\n",
    "\n",
    "# load pickle file\n",
    "with open(\"gen_dgrl_data/caveflyer/caveflyer/caveflyer\", \"rb\") as f:\n",
    "    episodes = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
